{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_09.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_BsEJesxACRz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JwSwNW9QmT"
      },
      "source": [
        "\n",
        "# CUDA Exercise 09\n",
        "> You should try to implement your own solution for matrix multiplication, and try to parallelize the computation.\n",
        "\n",
        "This Jupyter Notebook can also be open by the google colab, so you don't have to buy a PC with a graphic card to play with CUDA. To launch the Google Colab, please click the below Icon.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg#left)](https://colab.research.google.com/github/SuperChange001/CUDA_Learning/blob/main/Solution/Exercise_09.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEai4hb95Ip"
      },
      "source": [
        "## Initialize the CUDA dev environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqmwwI7H5nDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d963df04-926f-400a-d0e4-2878c4a03198"
      },
      "source": [
        "# clone the code repo,\n",
        "!pip install git+git://github.com/depctg/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/depctg/nvcc4jupyter.git\n",
            "  Cloning git://github.com/depctg/nvcc4jupyter.git to /tmp/pip-req-build-9uosm_fy\n",
            "  Running command git clone -q git://github.com/depctg/nvcc4jupyter.git /tmp/pip-req-build-9uosm_fy\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp37-none-any.whl size=4334 sha256=b2c7f0347c89a0d2f434e28ded0da15c6996ef06e1885e654b7568adf563eff6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kkvx15za/wheels/1e/43/2d/099cad2b9b02dfa88573f50a22735d8a0b2ba69bf82167b81c\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "Default out bin result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Zeyyo4_gNH"
      },
      "source": [
        "## Check the environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6PT4QpR6oxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d20d71f-ff18-4f85-a6e5-8c90e1f97a8a"
      },
      "source": [
        "!lsb_release -a\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 18.04.5 LTS\n",
            "Release:\t18.04\n",
            "Codename:\tbionic\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n",
            "Mon Apr 26 21:01:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF6KTYqE_n7H"
      },
      "source": [
        "## Matrix Multiplication - Implimentation 01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev5_BW1z80S3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286f06f4-0014-49b2-ac34-21640ade8975"
      },
      "source": [
        "%%writefile matrix_mul_01.cu\n",
        "// %%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void matrix_mul(int *matrix_a, int *matrix_b, int *matrix_c,int matrix_a_row,int matrix_a_column,int matrix_b_column){\n",
        "    int matrix_c_element = 0;\n",
        "    for (int i = 0; i < matrix_a_column; i++){\n",
        "      matrix_c_element += matrix_a[(threadIdx.x/matrix_b_column)*matrix_a_column+i] * matrix_b[threadIdx.x%matrix_b_column+i*matrix_b_column];\n",
        "    }\n",
        "    matrix_c[threadIdx.x]= matrix_c_element;\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]){\n",
        "    \n",
        "    //===========================================================================\n",
        "    // Below, there are three example case, which you should only uncomment one\n",
        "    // of them, to run the test.\n",
        "    /* Example 1\n",
        "    int matrix_a[16] = {5,0,34,21,7,17,-12,28,8,-3,-3,-3,0,-3,5,9};\n",
        "    int matrix_a_row = 4;\n",
        "    int matrix_a_column = 4;\n",
        "    int matrix_b[16] = {0,16,24,-90,-23,0,11,1,3,3,0,3,66,7,8,0};\n",
        "    int matrix_b_row = 4;\n",
        "    int matrix_b_column = 4;\n",
        "    */\n",
        "\n",
        "    /* Example 2\n",
        "    int matrix_a[12] = {12,6,22,7,17,-12,36,9,9,0,-1,-2};\n",
        "    int matrix_a_row = 4;\n",
        "    int matrix_a_column = 3;\n",
        "    int matrix_b[15] = {0,16,24,-1,4,-23,0,11,1,4,3,3,0,3,4};\n",
        "    int matrix_b_row = 3;\n",
        "    int matrix_b_column = 5;\n",
        "    */\n",
        "\n",
        "    // random initialization of larger matrixes\n",
        "    // matrix_a_row * matrix_b_column <= 1024\n",
        "    int matrix_a_row = 50;\n",
        "    int matrix_a_column = 30;\n",
        "    int *matrix_a = (int*) malloc(sizeof(int) * (matrix_a_row * matrix_a_column));\n",
        "    for(int i = 0; i < matrix_a_row; i++){\n",
        "        for(int j = 0; j < matrix_a_column; j++)\n",
        "        {\n",
        "            int index = i * matrix_a_column+j;\n",
        "            matrix_a[index] = 1;\n",
        "        }\n",
        "    }\n",
        "    int matrix_b_row = 30;\n",
        "    int matrix_b_column = 20;\n",
        "    int *matrix_b = (int*) malloc(sizeof(int) * (matrix_b_row * matrix_b_column));\n",
        "    for(int i = 0; i < matrix_b_row; i++){\n",
        "        for(int j = 0; j < matrix_b_column; j++)\n",
        "        {\n",
        "            int index = i * matrix_b_column+j;\n",
        "            matrix_b[index] = 2;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    //===========================================================================\n",
        "\n",
        "    int *matrix_c = (int*) malloc(sizeof(int) * (matrix_a_row * matrix_b_column));\n",
        "    int *d_matrix_a, *d_matrix_b, *d_matrix_c;\n",
        "    \n",
        "    cudaMalloc((void**)&d_matrix_a,sizeof(int) * (matrix_a_row * matrix_a_column));\n",
        "    cudaMalloc((void**)&d_matrix_b,sizeof(int) * (matrix_b_row * matrix_b_column));\n",
        "    cudaMalloc((void**)&d_matrix_c,sizeof(int) * (matrix_a_row * matrix_b_column));\n",
        "\n",
        "    cudaMemcpy(d_matrix_a, matrix_a, sizeof(int) * (matrix_a_row * matrix_a_column), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_matrix_b, matrix_b, sizeof(int) * (matrix_b_row * matrix_b_column), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // implement 100 times for getting average execution time\n",
        "    for(int i=0; i<100;i++){\n",
        "      matrix_mul<<<1,matrix_a_row * matrix_b_column>>>(d_matrix_a, d_matrix_b, d_matrix_c, matrix_a_row,matrix_a_column, matrix_b_column);\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(matrix_c, d_matrix_c,sizeof(int) * (matrix_a_row * matrix_b_column), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // print matrix_c to check correction\n",
        "    for(int i = 0; i < matrix_a_row; i++){\n",
        "        for(int j = 0; j < matrix_b_column; j++){\n",
        "            int index = i * matrix_b_column +j;\n",
        "            printf(\"%d, \",matrix_c[index]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaFree(d_matrix_c);\n",
        "    cudaFree(d_matrix_b);\n",
        "    cudaFree(d_matrix_a);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing matrix_mul_01.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsEJesxACRz"
      },
      "source": [
        "## Evaluation to collect enough information for the benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjisNLsazjUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab265330-1331-44b7-a3ae-15f5334c006a"
      },
      "source": [
        "!nvcc -o matrix_mul_01 matrix_mul_01.cu\n",
        "!nvprof ./matrix_mul_01\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==165== NVPROF is profiling process 165, command: ./matrix_mul_01\n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "==165== Profiling application: ./matrix_mul_01\n",
            "==165== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.40%  1.1060ms       100  11.060us  10.944us  11.360us  matrix_mul(int*, int*, int*, int, int, int)\n",
            "                    0.35%  3.9360us         2  1.9680us  1.6320us  2.3040us  [CUDA memcpy HtoD]\n",
            "                    0.25%  2.7840us         1  2.7840us  2.7840us  2.7840us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.30%  339.36ms         3  113.12ms  3.2070us  339.35ms  cudaMalloc\n",
            "                    0.23%  779.42us         3  259.81us  10.308us  744.60us  cudaMemcpy\n",
            "                    0.14%  492.00us       100  4.9200us  3.7570us  35.725us  cudaLaunchKernel\n",
            "                    0.12%  400.87us         1  400.87us  400.87us  400.87us  cuDeviceGetPCIBusId\n",
            "                    0.11%  369.10us         1  369.10us  369.10us  369.10us  cuDeviceTotalMem\n",
            "                    0.06%  193.36us       101  1.9140us     144ns  76.246us  cuDeviceGetAttribute\n",
            "                    0.04%  131.28us         3  43.758us  4.4320us  115.95us  cudaFree\n",
            "                    0.01%  30.707us         1  30.707us  30.707us  30.707us  cuDeviceGetName\n",
            "                    0.00%  7.8160us         1  7.8160us  7.8160us  7.8160us  cudaDeviceSynchronize\n",
            "                    0.00%  1.9930us         3     664ns     216ns  1.3860us  cuDeviceGetCount\n",
            "                    0.00%  1.7150us         2     857ns     202ns  1.5130us  cuDeviceGet\n",
            "                    0.00%     295ns         1     295ns     295ns     295ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LefKVzj4VUV"
      },
      "source": [
        "## Matrix Multiplication - Implimentation 02"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZvzZt8d4UpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b64349-167d-4632-87a7-f1f8055b7afd"
      },
      "source": [
        "%%writefile matrix_mul_02.cu\n",
        "//%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void matrix_mul(int *matrix_a, int *matrix_b, int *matrix_c,int matrix_a_row,int matrix_a_column,int matrix_b_column){\n",
        "    int matrix_c_element = 0;\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    for (int i = 0; i < matrix_a_column; i++){\n",
        "      matrix_c_element += matrix_a[(tid/matrix_b_column)*matrix_a_column+i] * matrix_b[tid%matrix_b_column+i*matrix_b_column];\n",
        "    }\n",
        "    matrix_c[tid]= matrix_c_element;\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]){\n",
        "    \n",
        "    //===========================================================================\n",
        "    // Below, there are three example case, which you should only uncomment one\n",
        "    // of them, to run the test.\n",
        "\n",
        "    /* Example 1\n",
        "    int matrix_a[16] = {5,0,34,21,7,17,-12,28,8,-3,-3,-3,0,-3,5,9};\n",
        "    int matrix_a_row = 4;\n",
        "    int matrix_a_column = 4;\n",
        "    int matrix_b[16] = {0,16,24,-90,-23,0,11,1,3,3,0,3,66,7,8,0};\n",
        "    int matrix_b_row = 4;\n",
        "    int matrix_b_column = 4;\n",
        "    */\n",
        "    \n",
        "    /* Example 2\n",
        "    int matrix_a[12] = {12,6,22,7,17,-12,36,9,9,0,-1,-2};\n",
        "    int matrix_a_row = 4;\n",
        "    int matrix_a_column = 3;\n",
        "    int matrix_b[15] = {0,16,24,-1,4,-23,0,11,1,4,3,3,0,3,4};\n",
        "    int matrix_b_row = 3;\n",
        "    int matrix_b_column = 5;\n",
        "    */\n",
        "    \n",
        "    \n",
        "    // random initialization of larger matrixes\n",
        "    // matrix_a_row as number of blocks\n",
        "    // matrix_b_column as number of threads per block\n",
        "    int matrix_a_row = 50;\n",
        "    int matrix_a_column = 30;\n",
        "    int *matrix_a = (int*) malloc(sizeof(int) * (matrix_a_row * matrix_a_column));\n",
        "    for(int i = 0; i < matrix_a_row; i++){\n",
        "        for(int j = 0; j < matrix_a_column; j++)\n",
        "        {\n",
        "            int index = i * matrix_a_column+j;\n",
        "            matrix_a[index] = 1;\n",
        "        }\n",
        "    }\n",
        "    int matrix_b_row = 30;\n",
        "    int matrix_b_column = 20;\n",
        "    int *matrix_b = (int*) malloc(sizeof(int) * (matrix_b_row * matrix_b_column));\n",
        "    for(int i = 0; i < matrix_b_row; i++){\n",
        "        for(int j = 0; j < matrix_b_column; j++)\n",
        "        {\n",
        "            int index = i * matrix_b_column+j;\n",
        "            matrix_b[index] = 2;\n",
        "        }\n",
        "    }\n",
        "    //===========================================================================\n",
        "\n",
        "\n",
        "    int *matrix_c = (int*) malloc(sizeof(int) * (matrix_a_row * matrix_b_column));\n",
        "    int *d_matrix_a, *d_matrix_b, *d_matrix_c;\n",
        "    \n",
        "    cudaMalloc((void**)&d_matrix_a,sizeof(int) * (matrix_a_row * matrix_a_column));\n",
        "    cudaMalloc((void**)&d_matrix_b,sizeof(int) * (matrix_b_row * matrix_b_column));\n",
        "    cudaMalloc((void**)&d_matrix_c,sizeof(int) * (matrix_a_row * matrix_b_column));\n",
        "\n",
        "    cudaMemcpy(d_matrix_a, matrix_a, sizeof(int) * (matrix_a_row * matrix_a_column), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_matrix_b, matrix_b, sizeof(int) * (matrix_b_row * matrix_b_column), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // implement 100 times for getting average execution time\n",
        "    for(int i=0; i<100;i++){\n",
        "    matrix_mul<<<matrix_a_row,matrix_b_column>>>(d_matrix_a, d_matrix_b, d_matrix_c, matrix_a_row,matrix_a_column, matrix_b_column);\n",
        "    \n",
        "    //for comparison with 01.cu\n",
        "    //matrix_mul<<<1,matrix_a_row * matrix_b_column>>>(d_matrix_a, d_matrix_b, d_matrix_c, matrix_a_row,matrix_a_column, matrix_b_column);\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(matrix_c, d_matrix_c,sizeof(int) * (matrix_a_row * matrix_b_column), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // print matrix_c to check correction\n",
        "    for(int i = 0; i < matrix_a_row; i++){\n",
        "        for(int j = 0; j < matrix_b_column; j++){\n",
        "            int index = i * matrix_b_column +j;\n",
        "            printf(\"%d, \",matrix_c[index]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaFree(d_matrix_c);\n",
        "    cudaFree(d_matrix_b);\n",
        "    cudaFree(d_matrix_a);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing matrix_mul_02.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKNir-yF_F_8"
      },
      "source": [
        "## Evaluation to collect enough information for the benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s61EVRmqQ0RF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a411a83-bcd3-4549-fd3b-a87119bf81c3"
      },
      "source": [
        "!nvcc -o matrix_mul_02 matrix_mul_02.cu\n",
        "!nvprof ./matrix_mul_02"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==209== NVPROF is profiling process 209, command: ./matrix_mul_02\n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, \n",
            "==209== Profiling application: ./matrix_mul_02\n",
            "==209== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   98.67%  526.42us       100  5.2640us  5.1830us  5.6000us  matrix_mul(int*, int*, int*, int, int, int)\n",
            "                    0.78%  4.1600us         2  2.0800us  1.6320us  2.5280us  [CUDA memcpy HtoD]\n",
            "                    0.55%  2.9120us         1  2.9120us  2.9120us  2.9120us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.44%  256.98ms         3  85.659ms  3.3400us  256.97ms  cudaMalloc\n",
            "                    0.21%  536.21us       100  5.3620us  3.9960us  34.504us  cudaLaunchKernel\n",
            "                    0.15%  376.96us         1  376.96us  376.96us  376.96us  cuDeviceTotalMem\n",
            "                    0.07%  184.83us         3  61.610us  10.349us  156.72us  cudaMemcpy\n",
            "                    0.06%  157.59us         3  52.528us  3.3410us  145.02us  cudaFree\n",
            "                    0.06%  154.20us       101  1.5260us     143ns  69.320us  cuDeviceGetAttribute\n",
            "                    0.01%  28.938us         1  28.938us  28.938us  28.938us  cuDeviceGetName\n",
            "                    0.00%  7.2200us         1  7.2200us  7.2200us  7.2200us  cuDeviceGetPCIBusId\n",
            "                    0.00%  6.6720us         1  6.6720us  6.6720us  6.6720us  cudaDeviceSynchronize\n",
            "                    0.00%  1.8660us         3     622ns     220ns  1.2690us  cuDeviceGetCount\n",
            "                    0.00%  1.6280us         2     814ns     338ns  1.2900us  cuDeviceGet\n",
            "                    0.00%     294ns         1     294ns     294ns     294ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_08.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JwSwNW9QmT"
      },
      "source": [
        "\n",
        "# CUDA Exercise 08\n",
        "> You should try to implement your own solution for matrix vector multiplication, and try to parallelize the computation.\n",
        "\n",
        "This Jupyter Notebook can also be open by the google colab, so you don't have to buy a PC with a graphic card to play with CUDA. To launch the Google Colab, please click the below Icon.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg#left)](https://colab.research.google.com/github/SuperChange001/CUDA_Learning/blob/main/Solution/Exercise_08.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEai4hb95Ip"
      },
      "source": [
        "## Initialize the CUDA dev environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqmwwI7H5nDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4692fa-1acf-4689-ce13-f59b43ead1f6"
      },
      "source": [
        "# clone the code repo,\n",
        "!pip install git+git://github.com/depctg/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/depctg/nvcc4jupyter.git\n",
            "  Cloning git://github.com/depctg/nvcc4jupyter.git to /tmp/pip-req-build-6ri04v_g\n",
            "  Running command git clone -q git://github.com/depctg/nvcc4jupyter.git /tmp/pip-req-build-6ri04v_g\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp37-none-any.whl size=4334 sha256=4d14ae8e1b5d4553791c7785ff742a5ca7908444bfa86c9a7f151acbb55ff62c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-83ylvme0/wheels/1e/43/2d/099cad2b9b02dfa88573f50a22735d8a0b2ba69bf82167b81c\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "Default out bin result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Zeyyo4_gNH"
      },
      "source": [
        "## Check the environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6PT4QpR6oxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8244ea-c4d9-44fe-ba52-f106a709938f"
      },
      "source": [
        "!lsb_release -a\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 18.04.5 LTS\n",
            "Release:\t18.04\n",
            "Codename:\tbionic\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n",
            "Sun Apr 25 20:46:45 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF6KTYqE_n7H"
      },
      "source": [
        "## Naive approach of matrix vector multiplication\n",
        "Try to optimize it, you can do much better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev5_BW1z80S3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc6adc9-ccd6-46c1-9f1e-3b67a7e02e9f"
      },
      "source": [
        "%%writefile matrix_vector_multiplication.cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define M 100\n",
        "#define N 100\n",
        "#define MAX_ERR 1e-4\n",
        "\n",
        "__global__ void matrix_vector_multiplication(float* vector_result, float *matrix_a, float *vector_b, int m_row, int n_col) \n",
        "{\n",
        "    extern __shared__ float temp[];\n",
        " \n",
        "    // blockIdx.x => which row\n",
        "    // blockDim.x => row length\n",
        "    // threadIdx.x => which element in this row\n",
        " \n",
        "    // Unique tid which can index each single element in the matrix\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // the condiction logic make sure, we only do the calculation in the matrix space\n",
        "    int size_of_the_matrix = m_row*n_col;\n",
        "    if(tid<size_of_the_matrix)\n",
        "    {\n",
        "        temp[tid] = matrix_a[tid] * vector_b[threadIdx.x]; // sum\n",
        "    }\n",
        " \n",
        "    __syncthreads(); // synchronize all threads\n",
        " \n",
        "    // The accumulation only needs to happen at thread_0\n",
        "    if (threadIdx.x == 0)\n",
        "    {\n",
        "        float sum = 0;\n",
        "        int index = blockIdx.x * blockDim.x;\n",
        "        for (int i = index; i < index + blockDim.x ; i++)\n",
        "        {\n",
        "            sum += temp[i];\n",
        "        }\n",
        "        vector_result[blockIdx.x] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    float *martix_a, *martix_b, *vector_result;\n",
        "    float *d_martix_a, *d_martix_b, *d_vector_result;\n",
        " \n",
        "    martix_a = (float*)malloc(sizeof(float) * (M * N));\n",
        "    martix_b = (float*)malloc(sizeof(float) * (N));\n",
        "    vector_result = (float*)malloc(sizeof(float) * (M));\n",
        "\n",
        "    // data initializtion\n",
        "    for(int raw_num = 0; raw_num < M; raw_num++) \n",
        "    {\n",
        "        for(int col_num = 0; col_num < N; col_num++)\n",
        "        {\n",
        "            int index = raw_num*N+col_num;\n",
        "            martix_a[index] = raw_num*3.14f+col_num;\n",
        "        }\n",
        "    }\n",
        " \n",
        "    for(int col_num = 0; col_num < N; col_num++)\n",
        "    {\n",
        "        martix_b[col_num] = col_num+1;\n",
        "    }\n",
        " \n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_martix_a, sizeof(float) * (M * N));\n",
        "    cudaMalloc((void**)&d_martix_b, sizeof(float) * N);\n",
        "    cudaMalloc((void**)&d_vector_result, sizeof(float) * M);\n",
        "\n",
        "    // copy operator to GPU\n",
        "    cudaMemcpy(d_martix_a, martix_a, sizeof(float) * (M * N), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_martix_b, martix_b, sizeof(float) * N, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // GPU do the work, CPU waits\n",
        "    matrix_vector_multiplication<<<M,N,sizeof(float) * (M * N)>>>(d_vector_result, d_martix_a, d_martix_b, M, N);\n",
        " \n",
        "    // Get results from the GPU\n",
        "    cudaMemcpy(vector_result, d_vector_result, sizeof(float) * M, cudaMemcpyDeviceToHost);\n",
        " \n",
        "    // Test the result\n",
        "    for(int i = 0; i < M; i++)\n",
        "    {\n",
        "        float temp_sum =0;\n",
        "        for(int j = 0; j < N; j++)\n",
        "        {\n",
        "            int index = i*N+j;\n",
        "            temp_sum = temp_sum + martix_a[index]*martix_b[j]; \n",
        "        }\n",
        "        //printf(\"out[%d]: %f, %f\\n\", i, temp_sum, vector_result[i]);\n",
        "     \n",
        "        assert(fabs(vector_result[i] - temp_sum) < MAX_ERR);\n",
        "    }\n",
        "    printf(\"PASSED\\n\");\n",
        "\n",
        "    // Free the memory\n",
        "    cudaFree(d_martix_a);\n",
        "    cudaFree(d_martix_b);\n",
        "    cudaFree(d_vector_result);\n",
        "    free(martix_a);\n",
        "    free(martix_b);\n",
        "    free(vector_result);\n",
        "    \n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting matrix_vector_multiplication.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsEJesxACRz"
      },
      "source": [
        "## Evaluation to collect enough information for the benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjisNLsazjUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f37c2cd-23aa-42c1-ff7f-0e1ac2572987"
      },
      "source": [
        "!nvcc -o matrix_vector_multiplication matrix_vector_multiplication.cu\n",
        "!nvprof ./matrix_vector_multiplication 0 0\n",
        "!nvprof ./matrix_vector_multiplication 1 0\n",
        "!nvprof ./matrix_vector_multiplication 2 0\n",
        "!nvprof ./matrix_vector_multiplication 3 0\n",
        "!nvprof ./matrix_vector_multiplication 4 0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==166== NVPROF is profiling process 166, command: ./matrix_vector_multiplication 0 0\n",
            "PASSED\n",
            "==166== Profiling application: ./matrix_vector_multiplication 0 0\n",
            "==166== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   53.11%  9.5670us         1  9.5670us  9.5670us  9.5670us  matrix_vector_multiplication(float*, float*, float*, int, int)\n",
            "                   35.17%  6.3360us         2  3.1680us  1.4080us  4.9280us  [CUDA memcpy HtoD]\n",
            "                   11.72%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "      API calls:   98.77%  317.58ms         3  105.86ms  3.9820us  317.57ms  cudaMalloc\n",
            "                    0.98%  3.1590ms         1  3.1590ms  3.1590ms  3.1590ms  cuDeviceGetPCIBusId\n",
            "                    0.11%  360.55us         1  360.55us  360.55us  360.55us  cuDeviceTotalMem\n",
            "                    0.05%  146.39us       101  1.4490us     140ns  62.080us  cuDeviceGetAttribute\n",
            "                    0.04%  138.31us         3  46.102us  4.1920us  125.39us  cudaFree\n",
            "                    0.02%  66.555us         3  22.185us  12.925us  32.811us  cudaMemcpy\n",
            "                    0.01%  34.868us         1  34.868us  34.868us  34.868us  cudaLaunchKernel\n",
            "                    0.01%  30.340us         1  30.340us  30.340us  30.340us  cuDeviceGetName\n",
            "                    0.00%  1.6340us         2     817ns     288ns  1.3460us  cuDeviceGet\n",
            "                    0.00%  1.3770us         3     459ns     237ns     803ns  cuDeviceGetCount\n",
            "                    0.00%     276ns         1     276ns     276ns     276ns  cuDeviceGetUuid\n",
            "==177== NVPROF is profiling process 177, command: ./matrix_vector_multiplication 1 0\n",
            "PASSED\n",
            "==177== Profiling application: ./matrix_vector_multiplication 1 0\n",
            "==177== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   52.54%  9.5990us         1  9.5990us  9.5990us  9.5990us  matrix_vector_multiplication(float*, float*, float*, int, int)\n",
            "                   36.08%  6.5920us         2  3.2960us  1.6320us  4.9600us  [CUDA memcpy HtoD]\n",
            "                   11.38%  2.0800us         1  2.0800us  2.0800us  2.0800us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.60%  180.65ms         3  60.217ms  3.5120us  180.64ms  cudaMalloc\n",
            "                    0.19%  346.94us         1  346.94us  346.94us  346.94us  cuDeviceTotalMem\n",
            "                    0.08%  144.39us       101  1.4290us     139ns  57.541us  cuDeviceGetAttribute\n",
            "                    0.06%  108.45us         3  36.150us  4.3680us  94.815us  cudaFree\n",
            "                    0.04%  65.119us         3  21.706us  12.926us  31.528us  cudaMemcpy\n",
            "                    0.02%  30.745us         1  30.745us  30.745us  30.745us  cudaLaunchKernel\n",
            "                    0.01%  25.721us         1  25.721us  25.721us  25.721us  cuDeviceGetName\n",
            "                    0.00%  7.3610us         1  7.3610us  7.3610us  7.3610us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7910us         3     597ns     247ns     894ns  cuDeviceGetCount\n",
            "                    0.00%  1.3040us         2     652ns     326ns     978ns  cuDeviceGet\n",
            "                    0.00%     259ns         1     259ns     259ns     259ns  cuDeviceGetUuid\n",
            "==188== NVPROF is profiling process 188, command: ./matrix_vector_multiplication 2 0\n",
            "PASSED\n",
            "==188== Profiling application: ./matrix_vector_multiplication 2 0\n",
            "==188== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   53.20%  9.5680us         1  9.5680us  9.5680us  9.5680us  matrix_vector_multiplication(float*, float*, float*, int, int)\n",
            "                   35.05%  6.3040us         2  3.1520us  1.4080us  4.8960us  [CUDA memcpy HtoD]\n",
            "                   11.74%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.59%  179.94ms         3  59.980ms  3.5560us  179.93ms  cudaMalloc\n",
            "                    0.20%  360.13us         1  360.13us  360.13us  360.13us  cuDeviceTotalMem\n",
            "                    0.08%  138.49us       101  1.3710us     140ns  56.464us  cuDeviceGetAttribute\n",
            "                    0.06%  105.28us         3  35.094us  4.6640us  91.544us  cudaFree\n",
            "                    0.04%  73.811us         3  24.603us  13.530us  32.582us  cudaMemcpy\n",
            "                    0.02%  34.857us         1  34.857us  34.857us  34.857us  cuDeviceGetName\n",
            "                    0.01%  25.780us         1  25.780us  25.780us  25.780us  cudaLaunchKernel\n",
            "                    0.00%  5.4220us         1  5.4220us  5.4220us  5.4220us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.3330us         3     444ns     198ns     723ns  cuDeviceGetCount\n",
            "                    0.00%  1.1720us         2     586ns     299ns     873ns  cuDeviceGet\n",
            "                    0.00%     254ns         1     254ns     254ns     254ns  cuDeviceGetUuid\n",
            "==199== NVPROF is profiling process 199, command: ./matrix_vector_multiplication 3 0\n",
            "PASSED\n",
            "==199== Profiling application: ./matrix_vector_multiplication 3 0\n",
            "==199== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   53.21%  9.5680us         1  9.5680us  9.5680us  9.5680us  matrix_vector_multiplication(float*, float*, float*, int, int)\n",
            "                   35.23%  6.3350us         2  3.1670us  1.4070us  4.9280us  [CUDA memcpy HtoD]\n",
            "                   11.57%  2.0800us         1  2.0800us  2.0800us  2.0800us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.58%  178.32ms         3  59.438ms  3.5200us  178.31ms  cudaMalloc\n",
            "                    0.20%  351.78us         1  351.78us  351.78us  351.78us  cuDeviceTotalMem\n",
            "                    0.08%  140.43us         3  46.810us  4.5390us  127.32us  cudaFree\n",
            "                    0.08%  134.77us       101  1.3340us     137ns  57.331us  cuDeviceGetAttribute\n",
            "                    0.03%  62.196us         3  20.732us  14.455us  27.225us  cudaMemcpy\n",
            "                    0.02%  29.932us         1  29.932us  29.932us  29.932us  cuDeviceGetName\n",
            "                    0.01%  24.688us         1  24.688us  24.688us  24.688us  cudaLaunchKernel\n",
            "                    0.00%  4.7770us         1  4.7770us  4.7770us  4.7770us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.3490us         3     449ns     200ns     842ns  cuDeviceGetCount\n",
            "                    0.00%  1.1530us         2     576ns     254ns     899ns  cuDeviceGet\n",
            "                    0.00%     253ns         1     253ns     253ns     253ns  cuDeviceGetUuid\n",
            "==210== NVPROF is profiling process 210, command: ./matrix_vector_multiplication 4 0\n",
            "PASSED\n",
            "==210== Profiling application: ./matrix_vector_multiplication 4 0\n",
            "==210== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   53.37%  9.6320us         1  9.6320us  9.6320us  9.6320us  matrix_vector_multiplication(float*, float*, float*, int, int)\n",
            "                   34.93%  6.3040us         2  3.1520us  1.3760us  4.9280us  [CUDA memcpy HtoD]\n",
            "                   11.70%  2.1110us         1  2.1110us  2.1110us  2.1110us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.58%  177.20ms         3  59.067ms  3.6720us  177.19ms  cudaMalloc\n",
            "                    0.21%  366.81us         1  366.81us  366.81us  366.81us  cuDeviceTotalMem\n",
            "                    0.08%  136.60us       101  1.3520us     138ns  57.487us  cuDeviceGetAttribute\n",
            "                    0.06%  110.76us         3  36.921us  4.7200us  96.591us  cudaFree\n",
            "                    0.03%  61.611us         3  20.537us  13.440us  27.097us  cudaMemcpy\n",
            "                    0.02%  29.652us         1  29.652us  29.652us  29.652us  cuDeviceGetName\n",
            "                    0.02%  28.677us         1  28.677us  28.677us  28.677us  cudaLaunchKernel\n",
            "                    0.00%  6.0860us         1  6.0860us  6.0860us  6.0860us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.2890us         3     429ns     247ns     732ns  cuDeviceGetCount\n",
            "                    0.00%  1.1320us         2     566ns     280ns     852ns  cuDeviceGet\n",
            "                    0.00%     257ns         1     257ns     257ns     257ns  cuDeviceGetUuid\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
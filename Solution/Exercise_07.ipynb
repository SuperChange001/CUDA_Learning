{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JwSwNW9QmT"
      },
      "source": [
        "\n",
        "# CUDA Exercise 07\n",
        "> You should try to implement your own solution for vector dot product, and try to parallelize the computation.\n",
        "\n",
        "This Jupyter Notebook can also be open by the google colab, so you don't have to buy a PC with a graphic card to play with CUDA. To launch the Google Colab, please click the below.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)#left](https://colab.research.google.com/github/SuperChange001/CUDA_Learning/blob/main/Solution/Exercise_07.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEai4hb95Ip"
      },
      "source": [
        "## Initialize the CUDA dev environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqmwwI7H5nDx"
      },
      "source": [
        "# clone the code repo,\n",
        "!pip install git+git://github.com/depctg/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Zeyyo4_gNH"
      },
      "source": [
        "## Check the environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6PT4QpR6oxt"
      },
      "source": [
        "!lsb_release -a\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF6KTYqE_n7H"
      },
      "source": [
        "## Naive approach of vector dot product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev5_BW1z80S3"
      },
      "source": [
        "%%writefile exercise01.cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define MAX_ERR 0.1\n",
        "#define MULTI_TIMES_RUN 1\n",
        "\n",
        "__global__ void vector_dot_product(float *result, float *vector_a, float *vector_b, int vertor_length) \n",
        "{\n",
        "    extern __shared__ float temp[];\n",
        " \n",
        "    int index = threadIdx.x;    // index offset of this thread\n",
        "    int stride = blockDim.x;    // stride step of each iteration\n",
        "\n",
        "    // so if threadIdx.x=0, and blockDim.x=10,\n",
        "    // then this thread is responsible for calculating temp[0], temp[10], temp[20]\n",
        "    // similiarly, the following thread will calculate temp[1], temp[11], temp[21]\n",
        "    for(int i = index; i < vertor_length; i += stride)\n",
        "    {\n",
        "        temp[i] = vector_a[i] * vector_b[i];\n",
        "    }\n",
        " \n",
        "    __syncthreads(); // synchronize all threads\n",
        " \n",
        "    // The accumulation only needs to happen at thread_0\n",
        "    if (threadIdx.x == 0)\n",
        "    {\n",
        "        float sum = 0;\n",
        "        for (int i = 0; i < vertor_length; i++)\n",
        "        {\n",
        "            sum += temp[i];\n",
        "        }\n",
        "        *result=sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "    float *vector_a, *vector_b, *result;\n",
        "    float *d_vector_a, *d_vector_b, *d_result;\n",
        "    int list_of_thread_num[]={1,64,128,256,512,1024};\n",
        "    int list_of_vector_length[]={100,200,1000,2000,10000};\n",
        "    int thread_num = 1;\n",
        "    int vector_length = 1000;\n",
        " \n",
        "     if( argc == 3 ) {\n",
        "      //printf(\"The argument supplied is %s\\n\", argv[1]);\n",
        "      int arg1 = atoi(argv[1]);  //argv[0] is the program name\n",
        "                                //atoi = ascii to int\n",
        "      int arg2 = atoi(argv[2]);              \n",
        "      \n",
        "      vector_length = list_of_vector_length[arg1];\n",
        "      thread_num = list_of_thread_num[arg2];\n",
        "    }\n",
        "    else if( argc > 2 ) {\n",
        "      printf(\"Too many arguments supplied.\\n\");\n",
        "    }\n",
        "    else {\n",
        "      printf(\"One argument expected.\\n\");\n",
        "      \n",
        "    }\n",
        "\n",
        "    // Allocate memory on CPU\n",
        "    vector_a = (float*)malloc(sizeof(float) * vector_length);\n",
        "    vector_b = (float*)malloc(sizeof(float) * vector_length);\n",
        "    result = (float*)malloc(sizeof(float));\n",
        "\n",
        "    // data initializtion\n",
        "    for(int i = 0; i < vector_length; i++)\n",
        "    {\n",
        "        vector_a[i] = 0.1f;\n",
        "        vector_b[i] = 0.9f;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_vector_a, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_vector_b, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_result, sizeof(float));\n",
        "\n",
        "    // copy operator to GPU\n",
        "    cudaMemcpy(d_vector_a, vector_a, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_vector_b, vector_b, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // GPU do the work, CPU waits\n",
        "#if MULTI_TIMES_RUN\n",
        "    for(int i=0; i< 10; i++)\n",
        "    {\n",
        "#endif\n",
        "        vector_dot_product<<<1,thread_num,sizeof(float) * vector_length>>>(d_result, d_vector_a, d_vector_b, vector_length);\n",
        "#if MULTI_TIMES_RUN\n",
        "    }\n",
        " #endif\n",
        " \n",
        "    // Get results from the GPU\n",
        "    cudaMemcpy(result, d_result, sizeof(float), \n",
        "               cudaMemcpyDeviceToHost);\n",
        " \n",
        "    // Test the result\n",
        "    //assert(fabs(*result - vector_length*2*3.14) < MAX_ERR);\n",
        "    \n",
        "    // you only need them for checking if the math is correct\n",
        "     printf(\"result[0] = %f\\n\", result[0]);\n",
        "    // printf(\"PASSED\\n\");\n",
        "\n",
        "    // Free the memory\n",
        "    cudaFree(d_vector_a);\n",
        "    cudaFree(d_vector_b);\n",
        "    cudaFree(d_result);\n",
        "    free(vector_a);\n",
        "    free(vector_a);\n",
        "    free(result);\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unl0xR2C_27V"
      },
      "source": [
        "## Optimized approach of vector dot product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba05ukJC8AKq"
      },
      "source": [
        "%%writefile exercise01.cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define MAX_ERR 0.1\n",
        "#define MULTI_TIMES_RUN 1\n",
        "\n",
        "__global__ void vector_dot_product(float *result, float *vector_a, float *vector_b, int vertor_length) \n",
        "{\n",
        "    extern __shared__ float temp[];\n",
        " \n",
        "    int index = threadIdx.x;    // index offset of this thread\n",
        "    int stride = blockDim.x;    // stride step of each iteration\n",
        "\n",
        "    temp[threadIdx.x] = 0;\n",
        "    for(int i = index; i < vertor_length; i += stride)\n",
        "    {\n",
        "        temp[threadIdx.x] = temp[threadIdx.x] + vector_a[i] * vector_b[i];\n",
        "    }\n",
        " \n",
        "    __syncthreads(); // synchronize all threads\n",
        " \n",
        "    // The accumulation only needs to happen at thread_0\n",
        "    if (threadIdx.x == 0)\n",
        "    {\n",
        "        float sum = 0;\n",
        "        int thread_num = (vertor_length+blockDim.x)/blockDim.x;\n",
        "        for (int i = 0; i < thread_num; i++)\n",
        "        {\n",
        "            sum += temp[i];\n",
        "        }\n",
        "        *result=sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "    float *vector_a, *vector_b, *result;\n",
        "    float *d_vector_a, *d_vector_b, *d_result;\n",
        "    int list_of_thread_num[]={1,64,128,256,512,1024};\n",
        "    int list_of_vector_length[]={100,200,1000,2000,10000};\n",
        "    int thread_num = 1;\n",
        "    int vector_length = 1000;\n",
        " \n",
        "     if( argc == 3 ) {\n",
        "      //printf(\"The arguments supplied are %s, %s\\n\", argv[1], argv[2]);\n",
        "      int arg1 = atoi(argv[1]);  //argv[0] is the program name\n",
        "                                //atoi = ascii to int\n",
        "      int arg2 = atoi(argv[2]);              \n",
        "      \n",
        "      vector_length = list_of_vector_length[arg1];\n",
        "      thread_num = list_of_thread_num[arg2];\n",
        "    }\n",
        "    else if( argc > 2 ) {\n",
        "      printf(\"Too many arguments supplied.\\n\");\n",
        "    }\n",
        "    else {\n",
        "      printf(\"Two argument expected.\\n\");\n",
        "      return 0;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on CPU\n",
        "    vector_a = (float*)malloc(sizeof(float) * vector_length);\n",
        "    vector_b = (float*)malloc(sizeof(float) * vector_length);\n",
        "    result = (float*)malloc(sizeof(float));\n",
        "\n",
        "    // data initializtion\n",
        "    for(int i = 0; i < vector_length; i++)\n",
        "    {\n",
        "        vector_a[i] = 0.1f;\n",
        "        vector_b[i] = 0.9f;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_vector_a, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_vector_b, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_result, sizeof(float));\n",
        "\n",
        "    // copy operator to GPU\n",
        "    cudaMemcpy(d_vector_a, vector_a, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_vector_b, vector_b, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // GPU do the work, CPU waits\n",
        "#if MULTI_TIMES_RUN\n",
        "    for(int i=0; i< 10; i++)\n",
        "    {\n",
        "#endif\n",
        "        vector_dot_product<<<1,thread_num,sizeof(float) * thread_num>>>(d_result, d_vector_a, d_vector_b, vector_length);\n",
        "#if MULTI_TIMES_RUN\n",
        "    }\n",
        " #endif\n",
        " \n",
        "    // Get results from the GPU\n",
        "    cudaMemcpy(result, d_result, sizeof(float), \n",
        "               cudaMemcpyDeviceToHost);\n",
        " \n",
        "    // Test the result\n",
        "    //assert(fabs(*result - vector_length*2*3.14) < MAX_ERR);\n",
        "    \n",
        "    // you only need them for checking if the math is correct\n",
        "     printf(\"result[0] = %f\\n\", result[0]);\n",
        "    // printf(\"PASSED\\n\");\n",
        "\n",
        "    // Free the memory\n",
        "    cudaFree(d_vector_a);\n",
        "    cudaFree(d_vector_b);\n",
        "    cudaFree(d_result);\n",
        "    free(vector_a);\n",
        "    free(vector_a);\n",
        "    free(result);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsEJesxACRz"
      },
      "source": [
        "## Evaluation to collect enough information for the benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjisNLsazjUT"
      },
      "source": [
        "!nvcc -o exercise01 exercise01.cu\n",
        "!nvprof ./exercise01 0 0\n",
        "!nvprof ./exercise01 1 0\n",
        "!nvprof ./exercise01 2 0\n",
        "!nvprof ./exercise01 3 0\n",
        "!nvprof ./exercise01 4 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J20hMfub0Tr2"
      },
      "source": [
        "!nvcc -o exercise01 exercise01.cu\n",
        "!nvprof ./exercise01 4 0\n",
        "!nvprof ./exercise01 4 1\n",
        "!nvprof ./exercise01 4 2\n",
        "!nvprof ./exercise01 4 3\n",
        "!nvprof ./exercise01 4 4"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
